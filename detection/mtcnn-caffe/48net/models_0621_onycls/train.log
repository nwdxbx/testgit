I0621 13:05:14.912947 10834 caffe.cpp:218] Using GPUs 0
I0621 13:05:14.916874 10834 caffe.cpp:223] GPU 0: GeForce GTX 1080
I0621 13:05:15.121738 10834 solver.cpp:44] Initializing solver from parameters: 
base_lr: 1e-05
display: 5000
max_iter: 20000
lr_policy: "step"
gamma: 0.8
momentum: 0.9
weight_decay: 0.004
stepsize: 3000
snapshot: 5000
snapshot_prefix: "./models/48net"
solver_mode: GPU
device_id: 0
net: "train48.prototxt"
train_state {
  level: 0
  stage: ""
}
I0621 13:05:15.122172 10834 solver.cpp:87] Creating training net from net file: train48.prototxt
I0621 13:05:15.122481 10834 net.cpp:51] Initializing net from parameters: 
name: "face_48"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "PythonLayer"
  type: "Python"
  top: "data"
  top: "label"
  top: "roi"
  top: "pts"
  python_param {
    module: "pythonLayer"
    layer: "Data_Layer_train"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "prelu1"
  type: "PReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "prelu2"
  type: "PReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "prelu3"
  type: "PReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "prelu4"
  type: "PReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "InnerProduct"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "drop5"
  type: "Dropout"
  bottom: "conv5"
  top: "conv5"
  dropout_param {
    dropout_ratio: 0.25
  }
}
layer {
  name: "prelu5"
  type: "PReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "conv6-1"
  type: "InnerProduct"
  bottom: "conv5"
  top: "conv6-1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "cls_bridge"
  type: "Python"
  bottom: "conv6-1"
  bottom: "label"
  top: "conv6-1-valid"
  top: "label-valid"
  python_param {
    module: "pythonLayer"
    layer: "cls_Layer"
  }
}
layer {
  name: "ClassifyLoss"
  type: "SoftmaxWithLoss"
  bottom: "conv6-1-valid"
  bottom: "label-valid"
  top: "ClassifyLoss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
}
layer {
  name: "cls_Acc"
  type: "Accuracy"
  bottom: "conv6-1-valid"
  bottom: "label-valid"
  top: "cls_Acc"
  include {
    phase: TRAIN
  }
}
layer {
  name: "conv6-2"
  type: "InnerProduct"
  bottom: "conv5"
  top: "conv6-2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "RegressionLoss"
  type: "Python"
  bottom: "conv6-2"
  bottom: "roi"
  top: "RegressionLoss"
  loss_weight: 0.5
  python_param {
    module: "pythonLayer"
    layer: "regression_Layer"
  }
}
layer {
  name: "conv6-3"
  type: "InnerProduct"
  bottom: "conv5"
  top: "conv6-3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "LandmarkLoss"
  type: "Python"
  bottom: "conv6-3"
  bottom: "pts"
  top: "LandmarkLoss"
  loss_weight: 1
  python_param {
    module: "pythonLayer"
    layer: "regression_Layer"
  }
}
I0621 13:05:15.122738 10834 layer_factory.hpp:77] Creating layer PythonLayer
I0621 13:05:15.534325 10834 net.cpp:84] Creating Layer PythonLayer
I0621 13:05:15.534349 10834 net.cpp:380] PythonLayer -> data
I0621 13:05:15.534364 10834 net.cpp:380] PythonLayer -> label
I0621 13:05:15.534373 10834 net.cpp:380] PythonLayer -> roi
I0621 13:05:15.534377 10834 net.cpp:380] PythonLayer -> pts
Start Reading Classify Data into Memory...

20000  Classify Data have been read into Memory...
Start Reading Regression Data into Memory...

15000  Regression Data have been read into Memory...
Start Reading pts-regression Data into Memory...

10000  pts-regression Data have been read into Memory...
I0621 13:09:05.469251 10834 net.cpp:122] Setting up PythonLayer
I0621 13:09:05.471143 10834 net.cpp:129] Top shape: 64 3 48 48 (442368)
I0621 13:09:05.471151 10834 net.cpp:129] Top shape: 64 1 (64)
I0621 13:09:05.471154 10834 net.cpp:129] Top shape: 64 4 (256)
I0621 13:09:05.471156 10834 net.cpp:129] Top shape: 64 10 (640)
I0621 13:09:05.471158 10834 net.cpp:137] Memory required for data: 1773312
I0621 13:09:05.471171 10834 layer_factory.hpp:77] Creating layer conv1
I0621 13:09:05.471197 10834 net.cpp:84] Creating Layer conv1
I0621 13:09:05.471204 10834 net.cpp:406] conv1 <- data
I0621 13:09:05.471218 10834 net.cpp:380] conv1 -> conv1
I0621 13:09:05.808024 10834 net.cpp:122] Setting up conv1
I0621 13:09:05.808061 10834 net.cpp:129] Top shape: 64 32 46 46 (4333568)
I0621 13:09:05.808066 10834 net.cpp:137] Memory required for data: 19107584
I0621 13:09:05.808104 10834 layer_factory.hpp:77] Creating layer prelu1
I0621 13:09:05.808127 10834 net.cpp:84] Creating Layer prelu1
I0621 13:09:05.808145 10834 net.cpp:406] prelu1 <- conv1
I0621 13:09:05.808151 10834 net.cpp:367] prelu1 -> conv1 (in-place)
I0621 13:09:05.808784 10834 net.cpp:122] Setting up prelu1
I0621 13:09:05.808807 10834 net.cpp:129] Top shape: 64 32 46 46 (4333568)
I0621 13:09:05.808810 10834 net.cpp:137] Memory required for data: 36441856
I0621 13:09:05.808831 10834 layer_factory.hpp:77] Creating layer pool1
I0621 13:09:05.808837 10834 net.cpp:84] Creating Layer pool1
I0621 13:09:05.808841 10834 net.cpp:406] pool1 <- conv1
I0621 13:09:05.808845 10834 net.cpp:380] pool1 -> pool1
I0621 13:09:05.808881 10834 net.cpp:122] Setting up pool1
I0621 13:09:05.808887 10834 net.cpp:129] Top shape: 64 32 23 23 (1083392)
I0621 13:09:05.808889 10834 net.cpp:137] Memory required for data: 40775424
I0621 13:09:05.808892 10834 layer_factory.hpp:77] Creating layer conv2
I0621 13:09:05.808899 10834 net.cpp:84] Creating Layer conv2
I0621 13:09:05.808902 10834 net.cpp:406] conv2 <- pool1
I0621 13:09:05.808908 10834 net.cpp:380] conv2 -> conv2
I0621 13:09:05.810592 10834 net.cpp:122] Setting up conv2
I0621 13:09:05.810602 10834 net.cpp:129] Top shape: 64 64 21 21 (1806336)
I0621 13:09:05.810619 10834 net.cpp:137] Memory required for data: 48000768
I0621 13:09:05.810626 10834 layer_factory.hpp:77] Creating layer prelu2
I0621 13:09:05.810631 10834 net.cpp:84] Creating Layer prelu2
I0621 13:09:05.810634 10834 net.cpp:406] prelu2 <- conv2
I0621 13:09:05.810638 10834 net.cpp:367] prelu2 -> conv2 (in-place)
I0621 13:09:05.810753 10834 net.cpp:122] Setting up prelu2
I0621 13:09:05.810758 10834 net.cpp:129] Top shape: 64 64 21 21 (1806336)
I0621 13:09:05.810775 10834 net.cpp:137] Memory required for data: 55226112
I0621 13:09:05.810778 10834 layer_factory.hpp:77] Creating layer pool2
I0621 13:09:05.810797 10834 net.cpp:84] Creating Layer pool2
I0621 13:09:05.810801 10834 net.cpp:406] pool2 <- conv2
I0621 13:09:05.810804 10834 net.cpp:380] pool2 -> pool2
I0621 13:09:05.810844 10834 net.cpp:122] Setting up pool2
I0621 13:09:05.810849 10834 net.cpp:129] Top shape: 64 64 10 10 (409600)
I0621 13:09:05.810852 10834 net.cpp:137] Memory required for data: 56864512
I0621 13:09:05.810854 10834 layer_factory.hpp:77] Creating layer conv3
I0621 13:09:05.810873 10834 net.cpp:84] Creating Layer conv3
I0621 13:09:05.810875 10834 net.cpp:406] conv3 <- pool2
I0621 13:09:05.810879 10834 net.cpp:380] conv3 -> conv3
I0621 13:09:05.811875 10834 net.cpp:122] Setting up conv3
I0621 13:09:05.811884 10834 net.cpp:129] Top shape: 64 64 8 8 (262144)
I0621 13:09:05.811902 10834 net.cpp:137] Memory required for data: 57913088
I0621 13:09:05.811906 10834 layer_factory.hpp:77] Creating layer prelu3
I0621 13:09:05.811926 10834 net.cpp:84] Creating Layer prelu3
I0621 13:09:05.811929 10834 net.cpp:406] prelu3 <- conv3
I0621 13:09:05.811933 10834 net.cpp:367] prelu3 -> conv3 (in-place)
I0621 13:09:05.812002 10834 net.cpp:122] Setting up prelu3
I0621 13:09:05.812019 10834 net.cpp:129] Top shape: 64 64 8 8 (262144)
I0621 13:09:05.812021 10834 net.cpp:137] Memory required for data: 58961664
I0621 13:09:05.812041 10834 layer_factory.hpp:77] Creating layer pool3
I0621 13:09:05.812046 10834 net.cpp:84] Creating Layer pool3
I0621 13:09:05.812077 10834 net.cpp:406] pool3 <- conv3
I0621 13:09:05.812083 10834 net.cpp:380] pool3 -> pool3
I0621 13:09:05.812121 10834 net.cpp:122] Setting up pool3
I0621 13:09:05.812125 10834 net.cpp:129] Top shape: 64 64 4 4 (65536)
I0621 13:09:05.812142 10834 net.cpp:137] Memory required for data: 59223808
I0621 13:09:05.812144 10834 layer_factory.hpp:77] Creating layer conv4
I0621 13:09:05.812150 10834 net.cpp:84] Creating Layer conv4
I0621 13:09:05.812153 10834 net.cpp:406] conv4 <- pool3
I0621 13:09:05.812157 10834 net.cpp:380] conv4 -> conv4
I0621 13:09:05.813349 10834 net.cpp:122] Setting up conv4
I0621 13:09:05.813357 10834 net.cpp:129] Top shape: 64 128 3 3 (73728)
I0621 13:09:05.813360 10834 net.cpp:137] Memory required for data: 59518720
I0621 13:09:05.813380 10834 layer_factory.hpp:77] Creating layer prelu4
I0621 13:09:05.813385 10834 net.cpp:84] Creating Layer prelu4
I0621 13:09:05.813388 10834 net.cpp:406] prelu4 <- conv4
I0621 13:09:05.813392 10834 net.cpp:367] prelu4 -> conv4 (in-place)
I0621 13:09:05.813462 10834 net.cpp:122] Setting up prelu4
I0621 13:09:05.813467 10834 net.cpp:129] Top shape: 64 128 3 3 (73728)
I0621 13:09:05.813470 10834 net.cpp:137] Memory required for data: 59813632
I0621 13:09:05.813488 10834 layer_factory.hpp:77] Creating layer conv5
I0621 13:09:05.813494 10834 net.cpp:84] Creating Layer conv5
I0621 13:09:05.813496 10834 net.cpp:406] conv5 <- conv4
I0621 13:09:05.813501 10834 net.cpp:380] conv5 -> conv5
I0621 13:09:05.814666 10834 net.cpp:122] Setting up conv5
I0621 13:09:05.814671 10834 net.cpp:129] Top shape: 64 256 (16384)
I0621 13:09:05.814673 10834 net.cpp:137] Memory required for data: 59879168
I0621 13:09:05.814678 10834 layer_factory.hpp:77] Creating layer drop5
I0621 13:09:05.814683 10834 net.cpp:84] Creating Layer drop5
I0621 13:09:05.814687 10834 net.cpp:406] drop5 <- conv5
I0621 13:09:05.814690 10834 net.cpp:367] drop5 -> conv5 (in-place)
I0621 13:09:05.814709 10834 net.cpp:122] Setting up drop5
I0621 13:09:05.814713 10834 net.cpp:129] Top shape: 64 256 (16384)
I0621 13:09:05.814716 10834 net.cpp:137] Memory required for data: 59944704
I0621 13:09:05.814719 10834 layer_factory.hpp:77] Creating layer prelu5
I0621 13:09:05.814723 10834 net.cpp:84] Creating Layer prelu5
I0621 13:09:05.814726 10834 net.cpp:406] prelu5 <- conv5
I0621 13:09:05.814729 10834 net.cpp:367] prelu5 -> conv5 (in-place)
I0621 13:09:05.814779 10834 net.cpp:122] Setting up prelu5
I0621 13:09:05.814784 10834 net.cpp:129] Top shape: 64 256 (16384)
I0621 13:09:05.814785 10834 net.cpp:137] Memory required for data: 60010240
I0621 13:09:05.814790 10834 layer_factory.hpp:77] Creating layer conv5_prelu5_0_split
I0621 13:09:05.814795 10834 net.cpp:84] Creating Layer conv5_prelu5_0_split
I0621 13:09:05.814797 10834 net.cpp:406] conv5_prelu5_0_split <- conv5
I0621 13:09:05.814801 10834 net.cpp:380] conv5_prelu5_0_split -> conv5_prelu5_0_split_0
I0621 13:09:05.814806 10834 net.cpp:380] conv5_prelu5_0_split -> conv5_prelu5_0_split_1
I0621 13:09:05.814811 10834 net.cpp:380] conv5_prelu5_0_split -> conv5_prelu5_0_split_2
I0621 13:09:05.814841 10834 net.cpp:122] Setting up conv5_prelu5_0_split
I0621 13:09:05.814846 10834 net.cpp:129] Top shape: 64 256 (16384)
I0621 13:09:05.814849 10834 net.cpp:129] Top shape: 64 256 (16384)
I0621 13:09:05.814852 10834 net.cpp:129] Top shape: 64 256 (16384)
I0621 13:09:05.814855 10834 net.cpp:137] Memory required for data: 60206848
I0621 13:09:05.814857 10834 layer_factory.hpp:77] Creating layer conv6-1
I0621 13:09:05.814863 10834 net.cpp:84] Creating Layer conv6-1
I0621 13:09:05.814867 10834 net.cpp:406] conv6-1 <- conv5_prelu5_0_split_0
I0621 13:09:05.814872 10834 net.cpp:380] conv6-1 -> conv6-1
I0621 13:09:05.814937 10834 net.cpp:122] Setting up conv6-1
I0621 13:09:05.814942 10834 net.cpp:129] Top shape: 64 2 (128)
I0621 13:09:05.814945 10834 net.cpp:137] Memory required for data: 60207360
I0621 13:09:05.814952 10834 layer_factory.hpp:77] Creating layer cls_bridge
I0621 13:09:05.814996 10834 net.cpp:84] Creating Layer cls_bridge
I0621 13:09:05.814999 10834 net.cpp:406] cls_bridge <- conv6-1
I0621 13:09:05.815011 10834 net.cpp:406] cls_bridge <- label
I0621 13:09:05.815016 10834 net.cpp:380] cls_bridge -> conv6-1-valid
I0621 13:09:05.815023 10834 net.cpp:380] cls_bridge -> label-valid
I0621 13:09:05.815132 10834 net.cpp:122] Setting up cls_bridge
I0621 13:09:05.815140 10834 net.cpp:129] Top shape: 64 2 (128)
I0621 13:09:05.815143 10834 net.cpp:129] Top shape: 64 1 (64)
I0621 13:09:05.815145 10834 net.cpp:137] Memory required for data: 60208128
I0621 13:09:05.815148 10834 layer_factory.hpp:77] Creating layer conv6-1-valid_cls_bridge_0_split
I0621 13:09:05.815152 10834 net.cpp:84] Creating Layer conv6-1-valid_cls_bridge_0_split
I0621 13:09:05.815156 10834 net.cpp:406] conv6-1-valid_cls_bridge_0_split <- conv6-1-valid
I0621 13:09:05.815160 10834 net.cpp:380] conv6-1-valid_cls_bridge_0_split -> conv6-1-valid_cls_bridge_0_split_0
I0621 13:09:05.815166 10834 net.cpp:380] conv6-1-valid_cls_bridge_0_split -> conv6-1-valid_cls_bridge_0_split_1
I0621 13:09:05.815189 10834 net.cpp:122] Setting up conv6-1-valid_cls_bridge_0_split
I0621 13:09:05.815194 10834 net.cpp:129] Top shape: 64 2 (128)
I0621 13:09:05.815197 10834 net.cpp:129] Top shape: 64 2 (128)
I0621 13:09:05.815199 10834 net.cpp:137] Memory required for data: 60209152
I0621 13:09:05.815202 10834 layer_factory.hpp:77] Creating layer label-valid_cls_bridge_1_split
I0621 13:09:05.815207 10834 net.cpp:84] Creating Layer label-valid_cls_bridge_1_split
I0621 13:09:05.815209 10834 net.cpp:406] label-valid_cls_bridge_1_split <- label-valid
I0621 13:09:05.815213 10834 net.cpp:380] label-valid_cls_bridge_1_split -> label-valid_cls_bridge_1_split_0
I0621 13:09:05.815218 10834 net.cpp:380] label-valid_cls_bridge_1_split -> label-valid_cls_bridge_1_split_1
I0621 13:09:05.815240 10834 net.cpp:122] Setting up label-valid_cls_bridge_1_split
I0621 13:09:05.815244 10834 net.cpp:129] Top shape: 64 1 (64)
I0621 13:09:05.815248 10834 net.cpp:129] Top shape: 64 1 (64)
I0621 13:09:05.815250 10834 net.cpp:137] Memory required for data: 60209664
I0621 13:09:05.815253 10834 layer_factory.hpp:77] Creating layer ClassifyLoss
I0621 13:09:05.815258 10834 net.cpp:84] Creating Layer ClassifyLoss
I0621 13:09:05.815261 10834 net.cpp:406] ClassifyLoss <- conv6-1-valid_cls_bridge_0_split_0
I0621 13:09:05.815264 10834 net.cpp:406] ClassifyLoss <- label-valid_cls_bridge_1_split_0
I0621 13:09:05.815269 10834 net.cpp:380] ClassifyLoss -> ClassifyLoss
I0621 13:09:05.815276 10834 layer_factory.hpp:77] Creating layer ClassifyLoss
I0621 13:09:05.815471 10834 net.cpp:122] Setting up ClassifyLoss
I0621 13:09:05.815477 10834 net.cpp:129] Top shape: (1)
I0621 13:09:05.815480 10834 net.cpp:132]     with loss weight 1
I0621 13:09:05.815501 10834 net.cpp:137] Memory required for data: 60209668
I0621 13:09:05.815505 10834 layer_factory.hpp:77] Creating layer cls_Acc
I0621 13:09:05.815528 10834 net.cpp:84] Creating Layer cls_Acc
I0621 13:09:05.815532 10834 net.cpp:406] cls_Acc <- conv6-1-valid_cls_bridge_0_split_1
I0621 13:09:05.815536 10834 net.cpp:406] cls_Acc <- label-valid_cls_bridge_1_split_1
I0621 13:09:05.815541 10834 net.cpp:380] cls_Acc -> cls_Acc
I0621 13:09:05.815549 10834 net.cpp:122] Setting up cls_Acc
I0621 13:09:05.815553 10834 net.cpp:129] Top shape: (1)
I0621 13:09:05.815557 10834 net.cpp:137] Memory required for data: 60209672
I0621 13:09:05.815572 10834 layer_factory.hpp:77] Creating layer conv6-2
I0621 13:09:05.815577 10834 net.cpp:84] Creating Layer conv6-2
I0621 13:09:05.815579 10834 net.cpp:406] conv6-2 <- conv5_prelu5_0_split_1
I0621 13:09:05.815584 10834 net.cpp:380] conv6-2 -> conv6-2
I0621 13:09:05.815657 10834 net.cpp:122] Setting up conv6-2
I0621 13:09:05.815662 10834 net.cpp:129] Top shape: 64 4 (256)
I0621 13:09:05.815665 10834 net.cpp:137] Memory required for data: 60210696
I0621 13:09:05.815670 10834 layer_factory.hpp:77] Creating layer RegressionLoss
I0621 13:09:05.815687 10834 net.cpp:84] Creating Layer RegressionLoss
I0621 13:09:05.815691 10834 net.cpp:406] RegressionLoss <- conv6-2
I0621 13:09:05.815696 10834 net.cpp:406] RegressionLoss <- roi
I0621 13:09:05.815706 10834 net.cpp:380] RegressionLoss -> RegressionLoss
I0621 13:09:05.815809 10834 net.cpp:122] Setting up RegressionLoss
I0621 13:09:05.815816 10834 net.cpp:129] Top shape: 1 (1)
I0621 13:09:05.815819 10834 net.cpp:132]     with loss weight 0.5
I0621 13:09:05.815824 10834 net.cpp:137] Memory required for data: 60210700
I0621 13:09:05.815826 10834 layer_factory.hpp:77] Creating layer conv6-3
I0621 13:09:05.815832 10834 net.cpp:84] Creating Layer conv6-3
I0621 13:09:05.815835 10834 net.cpp:406] conv6-3 <- conv5_prelu5_0_split_2
I0621 13:09:05.815840 10834 net.cpp:380] conv6-3 -> conv6-3
I0621 13:09:05.815918 10834 net.cpp:122] Setting up conv6-3
I0621 13:09:05.815922 10834 net.cpp:129] Top shape: 64 10 (640)
I0621 13:09:05.815927 10834 net.cpp:137] Memory required for data: 60213260
I0621 13:09:05.815930 10834 layer_factory.hpp:77] Creating layer LandmarkLoss
I0621 13:09:05.815944 10834 net.cpp:84] Creating Layer LandmarkLoss
I0621 13:09:05.815948 10834 net.cpp:406] LandmarkLoss <- conv6-3
I0621 13:09:05.815951 10834 net.cpp:406] LandmarkLoss <- pts
I0621 13:09:05.815955 10834 net.cpp:380] LandmarkLoss -> LandmarkLoss
I0621 13:09:05.816031 10834 net.cpp:122] Setting up LandmarkLoss
I0621 13:09:05.816038 10834 net.cpp:129] Top shape: 1 (1)
I0621 13:09:05.816041 10834 net.cpp:132]     with loss weight 1
I0621 13:09:05.816045 10834 net.cpp:137] Memory required for data: 60213264
I0621 13:09:05.816048 10834 net.cpp:198] LandmarkLoss needs backward computation.
I0621 13:09:05.816056 10834 net.cpp:198] conv6-3 needs backward computation.
I0621 13:09:05.816058 10834 net.cpp:198] RegressionLoss needs backward computation.
I0621 13:09:05.816061 10834 net.cpp:198] conv6-2 needs backward computation.
I0621 13:09:05.816066 10834 net.cpp:200] cls_Acc does not need backward computation.
I0621 13:09:05.816068 10834 net.cpp:198] ClassifyLoss needs backward computation.
I0621 13:09:05.816072 10834 net.cpp:200] label-valid_cls_bridge_1_split does not need backward computation.
I0621 13:09:05.816076 10834 net.cpp:198] conv6-1-valid_cls_bridge_0_split needs backward computation.
I0621 13:09:05.816079 10834 net.cpp:198] cls_bridge needs backward computation.
I0621 13:09:05.816082 10834 net.cpp:198] conv6-1 needs backward computation.
I0621 13:09:05.816087 10834 net.cpp:198] conv5_prelu5_0_split needs backward computation.
I0621 13:09:05.816089 10834 net.cpp:198] prelu5 needs backward computation.
I0621 13:09:05.816092 10834 net.cpp:198] drop5 needs backward computation.
I0621 13:09:05.816094 10834 net.cpp:198] conv5 needs backward computation.
I0621 13:09:05.816098 10834 net.cpp:198] prelu4 needs backward computation.
I0621 13:09:05.816102 10834 net.cpp:198] conv4 needs backward computation.
I0621 13:09:05.816104 10834 net.cpp:198] pool3 needs backward computation.
I0621 13:09:05.816107 10834 net.cpp:198] prelu3 needs backward computation.
I0621 13:09:05.816110 10834 net.cpp:198] conv3 needs backward computation.
I0621 13:09:05.816113 10834 net.cpp:198] pool2 needs backward computation.
I0621 13:09:05.816117 10834 net.cpp:198] prelu2 needs backward computation.
I0621 13:09:05.816118 10834 net.cpp:198] conv2 needs backward computation.
I0621 13:09:05.816123 10834 net.cpp:198] pool1 needs backward computation.
I0621 13:09:05.816125 10834 net.cpp:198] prelu1 needs backward computation.
I0621 13:09:05.816128 10834 net.cpp:198] conv1 needs backward computation.
I0621 13:09:05.816131 10834 net.cpp:200] PythonLayer does not need backward computation.
I0621 13:09:05.816134 10834 net.cpp:242] This network produces output ClassifyLoss
I0621 13:09:05.816138 10834 net.cpp:242] This network produces output LandmarkLoss
I0621 13:09:05.816140 10834 net.cpp:242] This network produces output RegressionLoss
I0621 13:09:05.816143 10834 net.cpp:242] This network produces output cls_Acc
I0621 13:09:05.816159 10834 net.cpp:255] Network initialization done.
I0621 13:09:05.816205 10834 solver.cpp:56] Solver scaffolding done.
I0621 13:09:05.816692 10834 caffe.cpp:155] Finetuning from ./48net-only-cls.caffemodel
I0621 13:09:05.823307 10834 net.cpp:744] Ignoring source layer label_PythonLayer_1_split
I0621 13:09:05.823336 10834 net.cpp:744] Ignoring source layer relu1
I0621 13:09:05.823352 10834 net.cpp:744] Ignoring source layer relu2
I0621 13:09:05.823374 10834 net.cpp:744] Ignoring source layer relu3
I0621 13:09:05.823395 10834 net.cpp:744] Ignoring source layer relu4
I0621 13:09:05.823398 10834 net.cpp:744] Ignoring source layer fc5
I0621 13:09:05.823400 10834 net.cpp:744] Ignoring source layer relu5
I0621 13:09:05.823402 10834 net.cpp:744] Ignoring source layer fc6-1
I0621 13:09:05.823405 10834 net.cpp:744] Ignoring source layer fc6-1_fc6-1_0_split
I0621 13:09:05.823428 10834 caffe.cpp:248] Starting Optimization
I0621 13:09:05.823433 10834 solver.cpp:272] Solving face_48
I0621 13:09:05.823436 10834 solver.cpp:273] Learning Rate Policy: step
I0621 13:09:05.951308 10834 solver.cpp:218] Iteration 0 (8.27462e-23 iter/s, 0.127795s/5000 iters), loss = 2.31733
I0621 13:09:05.951341 10834 solver.cpp:237]     Train net output #0: ClassifyLoss = 0.703545 (* 1 = 0.703545 loss)
I0621 13:09:05.951349 10834 solver.cpp:237]     Train net output #1: LandmarkLoss = 1.58173 (* 1 = 1.58173 loss)
I0621 13:09:05.951354 10834 solver.cpp:237]     Train net output #2: RegressionLoss = 0.064125 (* 0.5 = 0.0320625 loss)
I0621 13:09:05.951357 10834 solver.cpp:237]     Train net output #3: cls_Acc = 0.173913
I0621 13:09:05.951369 10834 sgd_solver.cpp:105] Iteration 0, lr = 1e-05
I0621 13:09:31.335319 10834 solver.cpp:447] Snapshotting to binary proto file ./models/48net_iter_5000.caffemodel
I0621 13:09:31.342533 10834 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./models/48net_iter_5000.solverstate
I0621 13:09:31.351373 10834 solver.cpp:218] Iteration 5000 (196.85 iter/s, 25.4s/5000 iters), loss = 0.641326
I0621 13:09:31.351416 10834 solver.cpp:237]     Train net output #0: ClassifyLoss = 0.587435 (* 1 = 0.587435 loss)
I0621 13:09:31.351438 10834 solver.cpp:237]     Train net output #1: LandmarkLoss = 0.0246385 (* 1 = 0.0246385 loss)
I0621 13:09:31.351442 10834 solver.cpp:237]     Train net output #2: RegressionLoss = 0.0585043 (* 0.5 = 0.0292521 loss)
I0621 13:09:31.351446 10834 solver.cpp:237]     Train net output #3: cls_Acc = 0.708333
I0621 13:09:31.351451 10834 sgd_solver.cpp:105] Iteration 5000, lr = 8e-06
I0621 13:09:56.795964 10834 solver.cpp:447] Snapshotting to binary proto file ./models/48net_iter_10000.caffemodel
I0621 13:09:56.802253 10834 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./models/48net_iter_10000.solverstate
I0621 13:09:56.810830 10834 solver.cpp:218] Iteration 10000 (196.391 iter/s, 25.4594s/5000 iters), loss = 0.598918
I0621 13:09:56.810873 10834 solver.cpp:237]     Train net output #0: ClassifyLoss = 0.556349 (* 1 = 0.556349 loss)
I0621 13:09:56.810880 10834 solver.cpp:237]     Train net output #1: LandmarkLoss = 0.0185649 (* 1 = 0.0185649 loss)
I0621 13:09:56.810885 10834 solver.cpp:237]     Train net output #2: RegressionLoss = 0.0480082 (* 0.5 = 0.0240041 loss)
I0621 13:09:56.810889 10834 solver.cpp:237]     Train net output #3: cls_Acc = 0.75
I0621 13:09:56.810894 10834 sgd_solver.cpp:105] Iteration 10000, lr = 5.12e-06
I0621 13:10:22.360285 10834 solver.cpp:447] Snapshotting to binary proto file ./models/48net_iter_15000.caffemodel
I0621 13:10:22.366488 10834 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./models/48net_iter_15000.solverstate
I0621 13:10:22.375267 10834 solver.cpp:218] Iteration 15000 (195.585 iter/s, 25.5643s/5000 iters), loss = 0.597674
I0621 13:10:22.375310 10834 solver.cpp:237]     Train net output #0: ClassifyLoss = 0.549497 (* 1 = 0.549497 loss)
I0621 13:10:22.375332 10834 solver.cpp:237]     Train net output #1: LandmarkLoss = 0.0189482 (* 1 = 0.0189482 loss)
I0621 13:10:22.375337 10834 solver.cpp:237]     Train net output #2: RegressionLoss = 0.0584576 (* 0.5 = 0.0292288 loss)
I0621 13:10:22.375340 10834 solver.cpp:237]     Train net output #3: cls_Acc = 0.758621
I0621 13:10:22.375361 10834 sgd_solver.cpp:105] Iteration 15000, lr = 3.2768e-06
I0621 13:10:48.045573 10834 solver.cpp:447] Snapshotting to binary proto file ./models/48net_iter_20000.caffemodel
I0621 13:10:48.051883 10834 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./models/48net_iter_20000.solverstate
I0621 13:10:48.057159 10834 solver.cpp:310] Iteration 20000, loss = 0.588199
I0621 13:10:48.057178 10834 solver.cpp:315] Optimization Done.
I0621 13:10:48.057194 10834 caffe.cpp:259] Optimization Done.
